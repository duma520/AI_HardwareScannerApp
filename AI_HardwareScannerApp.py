import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import platform
import psutil
import GPUtil
import cpuinfo
import cv2
import torch
import json
from datetime import datetime

class HardwareScannerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("AIåŠ é€Ÿèƒ½åŠ›æ£€æµ‹å·¥å…· v1.0.3")
        self.root.geometry("800x600")
        
        # å…¨å±€æ ·å¼è®¾ç½®
        self.style = ttk.Style()
        self.style.configure('TFrame', background='#f0f0f0')
        self.style.configure('TLabel', background='#f0f0f0', font=('Arial', 10))
        self.style.configure('Title.TLabel', font=('Arial', 16, 'bold'))
        self.style.configure('Highlight.TLabel', foreground='blue')
        self.style.configure('Warning.TLabel', foreground='red')
        
        # ä¸»ç•Œé¢å¸ƒå±€
        self.create_widgets()
        
        # æ£€æµ‹ç»“æœå­˜å‚¨
        self.results = {
            "system": {},
            "cpu": {},
            "gpu": {},
            "ram": {},
            "ai_frameworks": {},
            "recommendations": []
        }

    def create_widgets(self):
        """åˆ›å»ºGUIç•Œé¢ç»„ä»¶"""
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # æ ‡é¢˜åŒºåŸŸ
        title_frame = ttk.Frame(main_frame)
        title_frame.pack(fill=tk.X, pady=10)
        ttk.Label(title_frame, text="AIåŠ é€Ÿèƒ½åŠ›å…¨é¢æ£€æµ‹", style='Title.TLabel').pack()
        
        # æ£€æµ‹æŒ‰é’®åŒºåŸŸ
        btn_frame = ttk.Frame(main_frame)
        btn_frame.pack(fill=tk.X, pady=10)
        
        ttk.Button(btn_frame, text="å¼€å§‹å…¨é¢æ£€æµ‹", command=self.run_full_scan).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="å¿«é€Ÿæ£€æµ‹", command=self.run_quick_scan).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="ä¿å­˜æŠ¥å‘Š", command=self.save_report).pack(side=tk.RIGHT, padx=5)
        
        # ç»“æœæ˜¾ç¤ºåŒºåŸŸ
        self.notebook = ttk.Notebook(main_frame)
        self.notebook.pack(fill=tk.BOTH, expand=True)
        
        # åˆ›å»ºå„ä¸ªæ ‡ç­¾é¡µ
        self.create_system_tab()
        self.create_cpu_tab()
        self.create_gpu_tab()
        self.create_ai_tab()
        self.create_recommendation_tab()
        
        # çŠ¶æ€æ 
        self.status_var = tk.StringVar()
        self.status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN)
        self.status_bar.pack(fill=tk.X, pady=(5,0))
        self.status_var.set("å°±ç»ª - ç‚¹å‡»å¼€å§‹æ£€æµ‹æŒ‰é’®")

    def create_system_tab(self):
        """ç³»ç»Ÿä¿¡æ¯æ ‡ç­¾é¡µ"""
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text="ç³»ç»Ÿä¿¡æ¯")
        
        # ä½¿ç”¨ç½‘æ ¼å¸ƒå±€
        for i in range(6): tab.grid_columnconfigure(i, weight=1)
        
        ttk.Label(tab, text="æ“ä½œç³»ç»Ÿ:").grid(row=0, column=0, sticky="e", padx=5, pady=2)
        self.os_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.os_label.grid(row=0, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="Pythonç‰ˆæœ¬:").grid(row=1, column=0, sticky="e", padx=5, pady=2)
        self.py_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.py_label.grid(row=1, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="OpenCVç‰ˆæœ¬:").grid(row=2, column=0, sticky="e", padx=5, pady=2)
        self.cv_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cv_label.grid(row=2, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="PyTorchç‰ˆæœ¬:").grid(row=3, column=0, sticky="e", padx=5, pady=2)
        self.torch_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.torch_label.grid(row=3, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="CUDAå¯ç”¨æ€§:").grid(row=4, column=0, sticky="e", padx=5, pady=2)
        self.cuda_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cuda_label.grid(row=4, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="Matplotlibç‰ˆæœ¬:").grid(row=5, column=0, sticky="e", padx=5, pady=2)
        self.matplotlib_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.matplotlib_label.grid(row=5, column=1, sticky="w", pady=2)

    def create_cpu_tab(self):
        """CPUä¿¡æ¯æ ‡ç­¾é¡µ"""
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text="CPUä¿¡æ¯")
        
        # CPUåŸºæœ¬ä¿¡æ¯
        ttk.Label(tab, text="å¤„ç†å™¨å‹å·:").grid(row=0, column=0, sticky="e", padx=5, pady=2)
        self.cpu_model_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cpu_model_label.grid(row=0, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="æ ¸å¿ƒæ•°é‡:").grid(row=1, column=0, sticky="e", padx=5, pady=2)
        self.cpu_cores_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cpu_cores_label.grid(row=1, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="çº¿ç¨‹æ•°é‡:").grid(row=2, column=0, sticky="e", padx=5, pady=2)
        self.cpu_threads_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cpu_threads_label.grid(row=2, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="åŸºå‡†é¢‘ç‡:").grid(row=3, column=0, sticky="e", padx=5, pady=2)
        self.cpu_freq_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cpu_freq_label.grid(row=3, column=1, sticky="w", pady=2)
        
        # CPUèƒ½åŠ›æ£€æµ‹
        ttk.Label(tab, text="AVXæŒ‡ä»¤é›†:").grid(row=4, column=0, sticky="e", padx=5, pady=2)
        self.avx_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.avx_label.grid(row=4, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="SSE4.2æŒ‡ä»¤é›†:").grid(row=5, column=0, sticky="e", padx=5, pady=2)
        self.sse_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.sse_label.grid(row=5, column=1, sticky="w", pady=2)

    def create_gpu_tab(self):
        """GPUä¿¡æ¯æ ‡ç­¾é¡µ"""
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text="GPUä¿¡æ¯")
        
        # GPUåŸºæœ¬ä¿¡æ¯
        ttk.Label(tab, text="æ˜¾å¡å‹å·:").grid(row=0, column=0, sticky="e", padx=5, pady=2)
        self.gpu_model_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.gpu_model_label.grid(row=0, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="æ˜¾å­˜å®¹é‡:").grid(row=1, column=0, sticky="e", padx=5, pady=2)
        self.gpu_mem_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.gpu_mem_label.grid(row=1, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="CUDAæ ¸å¿ƒæ•°:").grid(row=2, column=0, sticky="e", padx=5, pady=2)
        self.cuda_cores_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cuda_cores_label.grid(row=2, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="é©±åŠ¨ç‰ˆæœ¬:").grid(row=3, column=0, sticky="e", padx=5, pady=2)
        self.driver_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.driver_label.grid(row=3, column=1, sticky="w", pady=2)
        
        # OpenCV GPUæ”¯æŒ
        ttk.Label(tab, text="OpenCV CUDA:").grid(row=4, column=0, sticky="e", padx=5, pady=2)
        self.cv_cuda_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.cv_cuda_label.grid(row=4, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="OpenCLæ”¯æŒ:").grid(row=5, column=0, sticky="e", padx=5, pady=2)
        self.opencl_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.opencl_label.grid(row=5, column=1, sticky="w", pady=2)

    def create_ai_tab(self):
        """AIæ¡†æ¶æ”¯æŒæ ‡ç­¾é¡µ"""
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text="AIæ”¯æŒ")
        
        # PyTorchæ”¯æŒ
        ttk.Label(tab, text="PyTorch GPUåŠ é€Ÿ:").grid(row=0, column=0, sticky="e", padx=5, pady=2)
        self.torch_gpu_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.torch_gpu_label.grid(row=0, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="TensorRTå¯ç”¨æ€§:").grid(row=1, column=0, sticky="e", padx=5, pady=2)
        self.tensorrt_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.tensorrt_label.grid(row=1, column=1, sticky="w", pady=2)
        
        # ONNX Runtimeæ”¯æŒ
        ttk.Label(tab, text="ONNX Runtime:").grid(row=2, column=0, sticky="e", padx=5, pady=2)
        self.onnx_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.onnx_label.grid(row=2, column=1, sticky="w", pady=2)
        
        # æ¨¡å‹æ”¯æŒæ£€æµ‹
        ttk.Label(tab, text="YOLOv8æ”¯æŒ:").grid(row=3, column=0, sticky="e", padx=5, pady=2)
        self.yolo_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.yolo_label.grid(row=3, column=1, sticky="w", pady=2)
        
        ttk.Label(tab, text="MobileNetæ”¯æŒ:").grid(row=4, column=0, sticky="e", padx=5, pady=2)
        self.mobilenet_label = ttk.Label(tab, text="", style='Highlight.TLabel')
        self.mobilenet_label.grid(row=4, column=1, sticky="w", pady=2)

    def create_recommendation_tab(self):
        """æ¨èæ–¹æ¡ˆæ ‡ç­¾é¡µ"""
        tab = ttk.Frame(self.notebook)
        self.notebook.add(tab, text="ä¼˜åŒ–å»ºè®®")
        
        self.recommendation_text = tk.Text(tab, wrap=tk.WORD, height=15, font=('Arial', 10))
        self.recommendation_text.grid(row=0, column=0, sticky="nsew", padx=5, pady=5)
        
        scrollbar = ttk.Scrollbar(tab, orient="vertical", command=self.recommendation_text.yview)
        scrollbar.grid(row=0, column=1, sticky="ns")
        self.recommendation_text.config(yscrollcommand=scrollbar.set)
        
        tab.grid_columnconfigure(0, weight=1)
        tab.grid_rowconfigure(0, weight=1)

    def run_full_scan(self):
        """æ‰§è¡Œå…¨é¢ç¡¬ä»¶æ£€æµ‹"""
        self.status_var.set("æ­£åœ¨æ£€æµ‹ç³»ç»Ÿä¿¡æ¯...")
        self.root.update()
        
        try:
            # 1. æ£€æµ‹ç³»ç»Ÿä¿¡æ¯
            self.scan_system_info()
            
            # 2. æ£€æµ‹CPUä¿¡æ¯
            self.status_var.set("æ­£åœ¨æ£€æµ‹CPUä¿¡æ¯...")
            self.root.update()
            self.scan_cpu_info()
            
            # 3. æ£€æµ‹GPUä¿¡æ¯
            self.status_var.set("æ­£åœ¨æ£€æµ‹GPUä¿¡æ¯...")
            self.root.update()
            self.scan_gpu_info()
            
            # 4. æ£€æµ‹AIæ¡†æ¶æ”¯æŒ
            self.status_var.set("æ­£åœ¨æ£€æµ‹AIæ¡†æ¶æ”¯æŒ...")
            self.root.update()
            self.scan_ai_support()
            
            # 5. ç”Ÿæˆä¼˜åŒ–å»ºè®®
            self.status_var.set("æ­£åœ¨ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
            self.root.update()
            self.generate_recommendations()
            
            self.status_var.set("æ£€æµ‹å®Œæˆï¼")
            messagebox.showinfo("å®Œæˆ", "ç¡¬ä»¶æ£€æµ‹å·²å®Œæˆï¼Œè¯·æŸ¥çœ‹å„æ ‡ç­¾é¡µè¯¦æƒ…")
            
        except Exception as e:
            self.status_var.set(f"æ£€æµ‹å‡ºé”™: {str(e)}")
            messagebox.showerror("é”™è¯¯", f"æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:\n{str(e)}")

    def run_quick_scan(self):
        """æ‰§è¡Œå¿«é€Ÿæ£€æµ‹ï¼ˆä»…å…³é”®ä¿¡æ¯ï¼‰"""
        self.status_var.set("æ­£åœ¨å¿«é€Ÿæ£€æµ‹...")
        self.root.update()
        
        try:
            self.scan_system_info()
            self.scan_cpu_info()
            self.scan_gpu_info()
            self.generate_recommendations()
            
            self.status_var.set("å¿«é€Ÿæ£€æµ‹å®Œæˆï¼")
            messagebox.showinfo("å®Œæˆ", "å¿«é€Ÿæ£€æµ‹å·²å®Œæˆ")
        except Exception as e:
            self.status_var.set(f"å¿«é€Ÿæ£€æµ‹å‡ºé”™: {str(e)}")
            messagebox.showerror("é”™è¯¯", f"å¿«é€Ÿæ£€æµ‹å‡ºé”™:\n{str(e)}")

    def scan_system_info(self):
        """æ‰«æç³»ç»ŸåŸºæœ¬ä¿¡æ¯"""
        # æ“ä½œç³»ç»Ÿä¿¡æ¯
        system_info = {
            "system": platform.system(),
            "release": platform.release(),
            "version": platform.version(),
            "machine": platform.machine(),
            "python_version": platform.python_version()
        }
        
        self.os_label.config(text=f"{system_info['system']} {system_info['release']}")
        self.py_label.config(text=system_info['python_version'])
        
        # OpenCVä¿¡æ¯
        cv_info = {
            "version": cv2.__version__,
            "cuda": cv2.cuda.getCudaEnabledDeviceCount() > 0,
            "opencl": cv2.ocl.haveOpenCL()
        }
        
        self.cv_label.config(text=cv_info['version'])
        self.cv_cuda_label.config(text="æ”¯æŒ" if cv_info['cuda'] else "ä¸æ”¯æŒ")
        self.opencl_label.config(text="æ”¯æŒ" if cv_info['opencl'] else "ä¸æ”¯æŒ")
        
        # PyTorchä¿¡æ¯
        torch_info = {
            "version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else "æ— "
        }
        
        self.torch_label.config(text=torch_info['version'])
        self.cuda_label.config(text="å¯ç”¨" if torch_info['cuda_available'] else "ä¸å¯ç”¨")
        self.torch_gpu_label.config(text="å¯ç”¨" if torch_info['cuda_available'] else "ä¸å¯ç”¨")
        
        # Matplotlibä¿¡æ¯
        try:
            import matplotlib
            matplotlib_info = {
                "version": matplotlib.__version__,
                "font_cache": "å·²æ„å»º" if matplotlib.get_cachedir() else "æœªæ„å»º",
                "available": True
            }
            self.matplotlib_label.config(text=f"{matplotlib_info['version']} ({matplotlib_info['font_cache']})")
        except ImportError:
            matplotlib_info = {"available": False}
            self.matplotlib_label.config(text="æœªå®‰è£…")
        
        # å­˜å‚¨ç»“æœ
        self.results['system'] = system_info
        self.results['system']['opencv'] = cv_info
        self.results['system']['pytorch'] = torch_info
        self.results['system']['matplotlib'] = matplotlib_info

    def scan_cpu_info(self):
        """æ‰«æCPUä¿¡æ¯"""
        # ä½¿ç”¨cpuinfoè·å–è¯¦ç»†ä¿¡æ¯
        cpu_info = cpuinfo.get_cpu_info()
        
        # åŸºæœ¬ä¿¡æ¯
        self.cpu_model_label.config(text=cpu_info.get('brand_raw', 'æœªçŸ¥'))
        self.cpu_cores_label.config(text=psutil.cpu_count(logical=False))
        self.cpu_threads_label.config(text=psutil.cpu_count(logical=True))
        
        # CPUé¢‘ç‡
        freq = psutil.cpu_freq()
        self.cpu_freq_label.config(text=f"{freq.current:.2f} MHz (æœ€å¤§ {freq.max:.2f} MHz)")
        
        # æŒ‡ä»¤é›†æ”¯æŒ
        flags = cpu_info.get('flags', [])
        self.avx_label.config(text="æ”¯æŒ" if 'avx' in flags else "ä¸æ”¯æŒ")
        self.sse_label.config(text="æ”¯æŒ" if 'sse4_2' in flags else "ä¸æ”¯æŒ")
        
        # å­˜å‚¨ç»“æœ
        self.results['cpu'] = {
            "model": cpu_info.get('brand_raw', 'æœªçŸ¥'),
            "cores": psutil.cpu_count(logical=False),
            "threads": psutil.cpu_count(logical=True),
            "frequency": {
                "current": freq.current,
                "max": freq.max
            },
            "features": {
                "avx": 'avx' in flags,
                "sse4_2": 'sse4_2' in flags,
                "avx2": 'avx2' in flags,
                "fma": 'fma' in flags
            },
            "usage": psutil.cpu_percent(interval=1)
        }

    def scan_gpu_info(self):
        """æ‰«æGPUä¿¡æ¯"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # å–ç¬¬ä¸€ä¸ªGPU
                
                # åŸºæœ¬ä¿¡æ¯
                self.gpu_model_label.config(text=gpu.name)
                self.gpu_mem_label.config(text=f"{gpu.memoryTotal}MB")
                self.driver_label.config(text=gpu.driver)
                
                # CUDAæ ¸å¿ƒæ•°ï¼ˆä¼°ç®—ï¼‰
                cuda_cores = self.estimate_cuda_cores(gpu.name)
                self.cuda_cores_label.config(text=str(cuda_cores) if cuda_cores else "æœªçŸ¥")
                
                # å­˜å‚¨ç»“æœ
                self.results['gpu'] = {
                    "name": gpu.name,
                    "memory_total": gpu.memoryTotal,
                    "memory_free": gpu.memoryFree,
                    "driver": gpu.driver,
                    "cuda_cores": cuda_cores,
                    "load": gpu.load * 100,
                    "detected": True
                }
            else:
                self.gpu_model_label.config(text="æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPU")
                self.results['gpu'] = {"detected": False}
                
        except Exception as e:
            self.gpu_model_label.config(text=f"æ£€æµ‹å‡ºé”™: {str(e)}")
            self.results['gpu'] = {"error": str(e), "detected": False}

    def estimate_cuda_cores(self, gpu_name):
        """ä¼°ç®—CUDAæ ¸å¿ƒæ•°ï¼ˆåŸºäºå¸¸è§GPUå‹å·ï¼‰"""
        gpu_name = gpu_name.lower()
        
        # NVIDIA GPU CUDAæ ¸å¿ƒæ•°ä¼°ç®—
        if 'rtx 3090' in gpu_name: return 10496
        if 'rtx 3080' in gpu_name: return 8704
        if 'rtx 3070' in gpu_name: return 5888
        if 'rtx 3060' in gpu_name: return 3584
        if 'gtx 1660' in gpu_name: return 1408
        if 'gtx 1080' in gpu_name: return 2560
        
        # æ— æ³•è¯†åˆ«æ—¶è¿”å›None
        return None

    def scan_ai_support(self):
        """æ£€æµ‹AIæ¡†æ¶å’Œæ¨¡å‹æ”¯æŒæƒ…å†µ"""
        ai_support = {}
        
        # æ£€æµ‹ONNX Runtime
        try:
            import onnxruntime
            onnx_ver = getattr(onnxruntime, '__version__', 'æœªçŸ¥')
            providers = getattr(onnxruntime, 'get_available_providers', lambda: [])()
            ai_support['onnxruntime'] = {
                "version": onnx_ver,
                "gpu": 'CUDAExecutionProvider' in providers,
                "available": True
            }
            self.onnx_label.config(text=f"æ”¯æŒ (GPU: {'æ˜¯' if ai_support['onnxruntime']['gpu'] else 'å¦'})")
        except Exception as e:
            ai_support['onnxruntime'] = {
                "available": False,
                "error": str(e)
            }
            self.onnx_label.config(text=f"é”™è¯¯: {str(e)}")
        
        # æ£€æµ‹TensorRT
        try:
            import tensorrt
            ai_support['tensorrt'] = {
                "version": getattr(tensorrt, '__version__', 'æœªçŸ¥'),
                "available": True
            }
            self.tensorrt_label.config(text=f"æ”¯æŒ (v{ai_support['tensorrt']['version']})")
        except Exception as e:
            ai_support['tensorrt'] = {
                "available": False,
                "error": str(e)
            }
            self.tensorrt_label.config(text=f"é”™è¯¯: {str(e)}")
        
        # æ£€æµ‹YOLOv8æ”¯æŒ
        try:
            from ultralytics import YOLO
            ai_support['yolov8'] = {"available": True}
            self.yolo_label.config(text="æ”¯æŒ")
        except ImportError as e:
            ai_support['yolov8'] = {
                "available": False,
                "error": str(e)
            }
            self.yolo_label.config(text="æœªå®‰è£…")
        
        # æ£€æµ‹MobileNetæ”¯æŒ
        try:
            import tensorflow as tf
            tf.keras.applications.MobileNetV2()
            ai_support['mobilenet'] = {"available": True}
            self.mobilenet_label.config(text="æ”¯æŒ")
        except Exception as e:
            ai_support['mobilenet'] = {
                "available": False,
                "error": str(e)
            }
            self.mobilenet_label.config(text="æœªå®‰è£…/ä¸æ”¯æŒ")
        
        self.results['ai_frameworks'] = ai_support

    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # CPUç›¸å…³å»ºè®®
        cpu = self.results['cpu']
        if cpu['features']['avx']:
            recommendations.append("âœ… æ‚¨çš„CPUæ”¯æŒAVXæŒ‡ä»¤é›†ï¼Œé€‚åˆè¿è¡Œä¼˜åŒ–åçš„OpenCVå’ŒNumPy")
        else:
            recommendations.append("âš ï¸ æ‚¨çš„CPUä¸æ”¯æŒAVXæŒ‡ä»¤é›†ï¼Œéƒ¨åˆ†AIåŠ é€ŸåŠŸèƒ½å¯èƒ½å—é™")
        
        if cpu['cores'] >= 4:
            recommendations.append(f"âœ… æ‚¨çš„CPUæœ‰{cpu['cores']}ä¸ªç‰©ç†æ ¸å¿ƒï¼Œå»ºè®®å¯ç”¨å¤šçº¿ç¨‹å¤„ç†")
        else:
            recommendations.append(f"âš ï¸ æ‚¨çš„CPUåªæœ‰{cpu['cores']}ä¸ªç‰©ç†æ ¸å¿ƒï¼Œå»ºè®®å‡å°‘å¹¶è¡Œä»»åŠ¡æ•°é‡")
        
        # GPUç›¸å…³å»ºè®®
        gpu = self.results['gpu']
        if 'detected' in gpu and not gpu['detected']:
            recommendations.append("âš ï¸ æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPUï¼Œå°†æ— æ³•ä½¿ç”¨CUDAåŠ é€Ÿ")
        elif 'name' in gpu:
            recommendations.append(f"âœ… æ£€æµ‹åˆ°GPU: {gpu['name']} (æ˜¾å­˜: {gpu['memory_total']}MB)")
            
            if gpu['memory_total'] < 2000:
                recommendations.append("âš ï¸ GPUæ˜¾å­˜è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨è½»é‡çº§æ¨¡å‹å¦‚YOLOv8næˆ–MobileNet")
            else:
                recommendations.append("âœ… æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥è¿è¡Œè¾ƒå¤§çš„æ¨¡å‹å¦‚YOLOv8x")
        
        # AIæ¡†æ¶å»ºè®®
        ai = self.results['ai_frameworks']
        if ai['onnxruntime'].get('available', False) and ai['onnxruntime'].get('gpu', False):
            recommendations.append("âœ… ONNX Runtimeå·²å®‰è£…å¹¶æ”¯æŒGPUåŠ é€Ÿï¼Œå»ºè®®ä½¿ç”¨ONNXæ ¼å¼æ¨¡å‹")
        
        if ai['tensorrt'].get('available', False):
            recommendations.append("âœ… TensorRTå·²å®‰è£…ï¼Œå¯å¯¹æ¨¡å‹è¿›è¡Œæè‡´ä¼˜åŒ–")
        
        # æ¨¡å‹é€‰æ‹©å»ºè®®
        if ai['yolov8'].get('available', False):
            recommendations.append("âœ… å·²å®‰è£…YOLOv8ï¼Œæ¨èä½¿ç”¨YOLOv8nè¿›è¡Œå®æ—¶ç›®æ ‡æ£€æµ‹")
        
        if ai['mobilenet'].get('available', False):
            recommendations.append("âœ… MobileNetå¯ç”¨ï¼Œé€‚åˆè½»é‡çº§å›¾åƒåˆ†ç±»ä»»åŠ¡")
        
        # æ ¹æ®ç¡¬ä»¶ç»„åˆæ¨èæœ€ä½³æ–¹æ¡ˆ
        if gpu.get('memory_total', 0) > 4000 and ai['tensorrt'].get('available', False):
            recommendations.append("ğŸš€ æ¨èæ–¹æ¡ˆ: YOLOv8 + TensorRTåŠ é€Ÿ (æœ€ä½³æ€§èƒ½)")
        elif gpu.get('memory_total', 0) > 2000:
            recommendations.append("ğŸ† æ¨èæ–¹æ¡ˆ: YOLOv8s + ONNX Runtime (å¹³è¡¡æ€§èƒ½)")
        else:
            recommendations.append("ğŸ› ï¸ æ¨èæ–¹æ¡ˆ: MobileNetV3 + OpenCV DNN (å…¼å®¹æ¨¡å¼)")
        
        # æ˜¾ç¤ºå»ºè®®
        self.recommendation_text.delete(1.0, tk.END)
        self.recommendation_text.insert(tk.END, "ç¡¬ä»¶ä¼˜åŒ–å»ºè®®:\n\n")
        
        for rec in recommendations:
            color = 'red' if 'âš ï¸' in rec else 'green' if 'âœ…' in rec else 'blue'
            self.recommendation_text.insert(tk.END, rec + "\n", color)
            self.recommendation_text.tag_config(color, foreground=color)
        
        self.results['recommendations'] = recommendations

    def save_report(self):
        """ä¿å­˜æ£€æµ‹æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not self.results.get('system'):
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆæ‰§è¡Œæ£€æµ‹å†ä¿å­˜æŠ¥å‘Š")
            return
        
        file_path = filedialog.asksaveasfilename(
            defaultextension=".json",
            filetypes=[("JSONæ–‡ä»¶", "*.json"), ("æ–‡æœ¬æ–‡ä»¶", "*.txt"), ("æ‰€æœ‰æ–‡ä»¶", "*.*")],
            title="ä¿å­˜æ£€æµ‹æŠ¥å‘Š"
        )
        
        if file_path:
            try:
                # æ·»åŠ ç”Ÿæˆæ—¶é—´
                self.results['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹ä¿å­˜
                if file_path.endswith('.json'):
                    with open(file_path, 'w') as f:
                        json.dump(self.results, f, indent=4)
                else:
                    with open(file_path, 'w') as f:
                        self.write_text_report(f)
                
                messagebox.showinfo("æˆåŠŸ", f"æŠ¥å‘Šå·²ä¿å­˜åˆ°:\n{file_path}")
            except Exception as e:
                messagebox.showerror("é”™è¯¯", f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™:\n{str(e)}")

    def write_text_report(self, file_handle):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        file_handle.write("="*50 + "\n")
        file_handle.write("AIåŠ é€Ÿèƒ½åŠ›æ£€æµ‹æŠ¥å‘Š\n")
        file_handle.write(f"ç”Ÿæˆæ—¶é—´: {self.results['timestamp']}\n")
        file_handle.write("="*50 + "\n\n")
        
        # ç³»ç»Ÿä¿¡æ¯
        file_handle.write("[ç³»ç»Ÿä¿¡æ¯]\n")
        sys_info = self.results['system']
        file_handle.write(f"æ“ä½œç³»ç»Ÿ: {sys_info['system']} {sys_info['release']}\n")
        file_handle.write(f"Pythonç‰ˆæœ¬: {sys_info['python_version']}\n")
        file_handle.write(f"OpenCVç‰ˆæœ¬: {sys_info['opencv']['version']} (CUDA: {'æ˜¯' if sys_info['opencv']['cuda'] else 'å¦'})\n")
        file_handle.write(f"PyTorchç‰ˆæœ¬: {sys_info['pytorch']['version']} (CUDA: {'æ˜¯' if sys_info['pytorch']['cuda_available'] else 'å¦'})\n")
        if 'matplotlib' in sys_info:
            file_handle.write(f"Matplotlibç‰ˆæœ¬: {sys_info['matplotlib'].get('version', 'æœªçŸ¥')} (å­—ä½“ç¼“å­˜: {sys_info['matplotlib'].get('font_cache', 'æœªçŸ¥')})\n")
        file_handle.write("\n")
        
        # CPUä¿¡æ¯
        file_handle.write("[CPUä¿¡æ¯]\n")
        cpu = self.results['cpu']
        file_handle.write(f"å‹å·: {cpu['model']}\n")
        file_handle.write(f"æ ¸å¿ƒ/çº¿ç¨‹: {cpu['cores']}/{cpu['threads']}\n")
        file_handle.write(f"å½“å‰é¢‘ç‡: {cpu['frequency']['current']} MHz\n")
        file_handle.write(f"æŒ‡ä»¤é›†: AVX({cpu['features']['avx']}) SSE4.2({cpu['features']['sse4_2']})\n\n")
        
        # GPUä¿¡æ¯
        file_handle.write("[GPUä¿¡æ¯]\n")
        gpu = self.results['gpu']
        if 'detected' in gpu and not gpu['detected']:
            file_handle.write("æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPU\n\n")
        else:
            file_handle.write(f"å‹å·: {gpu['name']}\n")
            file_handle.write(f"æ˜¾å­˜: {gpu['memory_total']} MB\n")
            file_handle.write(f"é©±åŠ¨ç‰ˆæœ¬: {gpu['driver']}\n")
            file_handle.write(f"CUDAæ ¸å¿ƒ: {gpu.get('cuda_cores', 'æœªçŸ¥')}\n\n")
        
        # AIæ¡†æ¶æ”¯æŒ
        file_handle.write("[AIæ¡†æ¶æ”¯æŒ]\n")
        ai = self.results['ai_frameworks']
        file_handle.write(f"ONNX Runtime: {'æ˜¯' if ai['onnxruntime'].get('available', False) else 'å¦'} (GPU: {'æ˜¯' if ai['onnxruntime'].get('gpu', False) else 'å¦'})\n")
        file_handle.write(f"TensorRT: {'æ˜¯' if ai['tensorrt'].get('available', False) else 'å¦'}\n")
        file_handle.write(f"YOLOv8: {'æ˜¯' if ai['yolov8'].get('available', False) else 'å¦'}\n")
        file_handle.write(f"MobileNet: {'æ˜¯' if ai['mobilenet'].get('available', False) else 'å¦'}\n\n")
        
        # ä¼˜åŒ–å»ºè®®
        file_handle.write("[ä¼˜åŒ–å»ºè®®]\n")
        for rec in self.results['recommendations']:
            file_handle.write(f"- {rec.replace('âœ…', '[æ¨è]').replace('âš ï¸', '[æ³¨æ„]')}\n")

if __name__ == "__main__":
    root = tk.Tk()
    app = HardwareScannerApp(root)
    root.mainloop()
